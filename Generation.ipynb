{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "710b3278",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dengjian/.conda/envs/507_gpu_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing for style: Romantic\n",
      "Loading from: trained_models/final_Romantic_gpt2\n",
      "Tokenizer loaded. Vocab size: 20000\n",
      "Model loaded on cuda\n",
      "Start Token ID: 0\n",
      "Setup complete. Run Cell 2 to generate.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "from miditok import REMI\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "# Change this to \"Baroque\" or \"Romantic\" to switch styles\n",
    "CURRENT_STYLE = \"Baroque\"\n",
    "\n",
    "if CURRENT_STYLE == \"Baroque\":\n",
    "    MODEL_PATH = \"trained_models/final_Baroque_gpt2\"\n",
    "else:\n",
    "    MODEL_PATH = \"trained_models/final_Romantic_gpt2\"\n",
    "\n",
    "# Generation Parameters\n",
    "MAX_LEN = 512\n",
    "MY_TEMP = 0.95\n",
    "MY_TOP_K = 45\n",
    "MY_TOP_P = 0.91\n",
    "MY_REPETITION_PENALTY = 1.08\n",
    "\n",
    "print(f\"Initializing for style: {CURRENT_STYLE}\")\n",
    "print(f\"Loading from: {MODEL_PATH}\")\n",
    "\n",
    "# Load Tokenizer\n",
    "try:\n",
    "    tokenizer = REMI.from_pretrained(MODEL_PATH)\n",
    "    print(f\"Tokenizer loaded. Vocab size: {len(tokenizer)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Tokenizer load error: {e}\")\n",
    "    raise e\n",
    "\n",
    "# Load Model\n",
    "try:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = GPT2LMHeadModel.from_pretrained(MODEL_PATH)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(f\"Model loaded on {device}\")\n",
    "except Exception as e:\n",
    "    print(f\"Model load error: {e}\")\n",
    "    raise e\n",
    "\n",
    "# Setup Start Token\n",
    "if getattr(tokenizer, \"bos_token_id\", None) is not None:\n",
    "    start_token = tokenizer.bos_token_id\n",
    "else:\n",
    "    start_token = 0\n",
    "\n",
    "print(f\"Start Token ID: {start_token}\")\n",
    "\n",
    "# Prepare input tensor for generation loop\n",
    "input_ids_template = torch.tensor([[start_token]], dtype=torch.long).to(device)\n",
    "\n",
    "print(\"Setup complete. Run Cell 2 to generate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7b56712",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=512) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch generation: Romantic_11 to Romantic_20\n",
      "\n",
      "Generating Romantic_11.mid (1/10)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1917102/3366195457.py:31: UserWarning: miditok: The `tokens_to_midi` method had been renamed `decode`. It is now depreciated and will be removed in future updates.\n",
      "  midi_obj = tokenizer.tokens_to_midi(generated_seq)\n",
      "/home/dengjian/.conda/envs/507_gpu_env/lib/python3.10/site-packages/miditok/midi_tokenizer.py:1944: UserWarning: The input sequence has one dimension less than expected (1 instead of 2). It is being unsqueezed to conform with the tokenizer's i/o format (('I', 'T'))\n",
      "  tokens = self._convert_sequence_to_tokseq(tokens)\n",
      "Both `max_new_tokens` (=512) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 513 tokens. Decoding...\n",
      "Saved (backup method): Romantic_11.mid\n",
      "\n",
      "Generating Romantic_12.mid (2/10)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=512) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 513 tokens. Decoding...\n",
      "Saved (backup method): Romantic_12.mid\n",
      "\n",
      "Generating Romantic_13.mid (3/10)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=512) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 513 tokens. Decoding...\n",
      "Saved (backup method): Romantic_13.mid\n",
      "\n",
      "Generating Romantic_14.mid (4/10)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=512) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 513 tokens. Decoding...\n",
      "Saved (backup method): Romantic_14.mid\n",
      "\n",
      "Generating Romantic_15.mid (5/10)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=512) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 513 tokens. Decoding...\n",
      "Saved (backup method): Romantic_15.mid\n",
      "\n",
      "Generating Romantic_16.mid (6/10)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=512) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 513 tokens. Decoding...\n",
      "Saved (backup method): Romantic_16.mid\n",
      "\n",
      "Generating Romantic_17.mid (7/10)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=512) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 513 tokens. Decoding...\n",
      "Saved (backup method): Romantic_17.mid\n",
      "\n",
      "Generating Romantic_18.mid (8/10)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=512) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 513 tokens. Decoding...\n",
      "Saved (backup method): Romantic_18.mid\n",
      "\n",
      "Generating Romantic_19.mid (9/10)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=512) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 513 tokens. Decoding...\n",
      "Saved (backup method): Romantic_19.mid\n",
      "\n",
      "Generating Romantic_20.mid (10/10)...\n",
      "Length: 513 tokens. Decoding...\n",
      "Saved (backup method): Romantic_20.mid\n",
      "\n",
      "Batch generation complete.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Batch Configuration\n",
    "START_INDEX = 11\n",
    "END_INDEX = 20\n",
    "\n",
    "print(f\"Starting batch generation: {CURRENT_STYLE}_{START_INDEX} to {CURRENT_STYLE}_{END_INDEX}\")\n",
    "\n",
    "for i in range(START_INDEX, END_INDEX + 1):\n",
    "    filename = f\"{CURRENT_STYLE}_{i}.mid\"\n",
    "    print(f\"\\nGenerating {filename} ({i - START_INDEX + 1}/{END_INDEX - START_INDEX + 1})...\")\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids_template,\n",
    "            max_length=MAX_LEN,\n",
    "            do_sample=True,\n",
    "            temperature=MY_TEMP,\n",
    "            top_k=MY_TOP_K,\n",
    "            top_p=MY_TOP_P,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            repetition_penalty=MY_REPETITION_PENALTY \n",
    "        )\n",
    "    \n",
    "    generated_seq = output_ids[0].cpu().tolist()\n",
    "    print(f\"Length: {len(generated_seq)} tokens. Decoding...\")\n",
    "\n",
    "    # Save logic handling different miditok versions\n",
    "    try:\n",
    "        midi_obj = tokenizer.tokens_to_midi(generated_seq)\n",
    "        midi_obj.dump(filename)\n",
    "        print(f\"Saved: {filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Fallback method\n",
    "        try:\n",
    "            midi_score = tokenizer.decode([generated_seq])\n",
    "            if hasattr(midi_score, \"dump\"):\n",
    "                midi_score.dump(filename)\n",
    "            elif hasattr(midi_score, \"dump_midi\"):\n",
    "                midi_score.dump_midi(filename)\n",
    "            else:\n",
    "                with open(filename, 'wb') as f:\n",
    "                    midi_score.write(f)\n",
    "            print(f\"Saved (backup method): {filename}\")\n",
    "            \n",
    "        except Exception as e_final:\n",
    "            print(f\"Failed to save {filename}: {e_final}\")\n",
    "\n",
    "print(\"\\nBatch generation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db0fd41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (507_gpu_env)",
   "language": "python",
   "name": "507_gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
